{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gc\n",
    "import os \n",
    "from time import time\n",
    "import sys\n",
    "from sklearn.model_selection import KFold, GroupKFold, train_test_split\n",
    "import json\n",
    "from numba import jit\n",
    "import re\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from pathlib import Path\n",
    "\n",
    "import scipy\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "\n",
    "# import modules\n",
    "# sys.path.append('../utils/')\n",
    "# from large_file_pickle import pickle_dump, pickle_load\n",
    "# # from notifications import send_line_notification, notify_slack\n",
    "# from memory_optimize import memory_reducer\n",
    "# from util import *\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from typing import List, Optional, Union, Tuple, Dict\n",
    "# from encoders import frequency_encoding\n",
    "from contextlib import contextmanager\n",
    "\n",
    "sys.path.append('../py/')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "pd.set_option('display.max_columns', 300)\n",
    "pd.set_option('display.max_colwidth', 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modules  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils\n",
    "\n",
    "### util function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def groupings(df, cols, agg_dict, pref='') -> object:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        object: \n",
    "    \"\"\"\n",
    "    group_df = df.groupby(cols).agg(agg_dict)\n",
    "    group_df.columns = [pref + c[0] + \"_\" + c[1] for c in list(group_df.columns)]\n",
    "    group_df.reset_index(inplace = True)\n",
    "    \n",
    "    return group_df\n",
    "\n",
    "@contextmanager\n",
    "def timer(name, logger=None):\n",
    "    \"\"\"時間計測\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    if logger:\n",
    "        logger.log(logging.DEBUG, f'[{name}] start')\n",
    "    else:\n",
    "        print(f'[{name}] start')\n",
    "    yield\n",
    "    if logger:\n",
    "        logger.log(logging.DEBUG, f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "    else:\n",
    "        print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "def get_val_score(y_true, y_pred, obj=\"RMSE\"):\n",
    "    # RMSE\n",
    "    if obj == \"RMSE\":\n",
    "        val_score = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    elif obj == \"QWK\":\n",
    "        val_score = qwk(y_true, y_pred, max_rat=3)\n",
    "    else:\n",
    "        raise ValueError(\"valuation is not defined!\")\n",
    "    return val_score\n",
    "\n",
    "def memory_reducer(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        print(col)\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max and c_prec == np.finfo(np.float16).precision:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def applyParallel(dfGrouped, func):\n",
    "    \"\"\"関数の並列処理\n",
    "    \"\"\"\n",
    "    retLst = Parallel(n_jobs=multiprocessing.cpu_count(), verbose=5)(delayed(func)(group) for name, group in dfGrouped)\n",
    "    return pd.concat(retLst)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm.callback import _format_eval_result\n",
    "\n",
    "def log_evaluation(logger, period=100, show_stdv=True, level=logging.DEBUG):\n",
    "    def _callback(env):\n",
    "        if period > 0 and env.evaluation_result_list and (env.iteration + 1) % period == 0:\n",
    "            result = '\\t'.join([_format_eval_result(x, show_stdv) for x in env.evaluation_result_list])\n",
    "            logger.log(level, '[{}]\\t{}'.format(env.iteration + 1, result))\n",
    "\n",
    "    _callback.order = 10\n",
    "    return _callback\n",
    "\n",
    "# ロガーの作成\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "def log_output(subject):\n",
    "    logger = logging.getLogger('main')\n",
    "    for h in logger.handlers:\n",
    "        logger.removeHandler(h)\n",
    "\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    now = int(time.time())\n",
    "\n",
    "    log_dir = os.path.join(os.path.dirname(\"__file__\"), \"../logs\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    log_path = Path(log_dir) / \"{}_{}.log\".format(subject, now)\n",
    "    fh = logging.FileHandler(log_path)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "    return logger, log_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MacOSFile(object):\n",
    "\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        return getattr(self.f, item)\n",
    "\n",
    "    def read(self, n):\n",
    "        # print(\"reading total_bytes=%s\" % n, flush=True)\n",
    "        if n >= (1 << 31):\n",
    "            buffer = bytearray(n)\n",
    "            idx = 0\n",
    "            while idx < n:\n",
    "                batch_size = min(n - idx, 1 << 31 - 1)\n",
    "                # print(\"reading bytes [%s,%s)...\" % (idx, idx + batch_size), end=\"\", flush=True)\n",
    "                buffer[idx:idx + batch_size] = self.f.read(batch_size)\n",
    "                # print(\"done.\", flush=True)\n",
    "                idx += batch_size\n",
    "            return buffer\n",
    "        return self.f.read(n)\n",
    "\n",
    "    def write(self, buffer):\n",
    "        n = len(buffer)\n",
    "\n",
    "        print(\"writing total_bytes=%s...\" % n, flush=True)\n",
    "        idx = 0\n",
    "        while idx < n:\n",
    "            # print(n, idx)\n",
    "            batch_size = min(n - idx, 1 << 31 - 1)\n",
    "            # print(batch_size)\n",
    "            # print(\"writing bytes [%s, %s)... \" % (idx, idx + batch_size), end=\"\", flush=True)\n",
    "            self.f.write(buffer[idx:idx + batch_size])\n",
    "            # print(\"done.\", flush=True)\n",
    "            idx += batch_size\n",
    "        print(\"calculate done!\")\n",
    "\n",
    "\n",
    "def pickle_dump(obj, file_path):\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        return pickle.dump(obj, MacOSFile(f), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def pickle_load(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return pickle.load(MacOSFile(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Features(metaclass=ABCMeta):\n",
    "\n",
    "    def __init__(self, params, logger=None):\n",
    "        self.name = self.__class__.__name__\n",
    "        self.datatype = params[\"datatype\"]\n",
    "        self.debug = params[\"debug\"]\n",
    "        self.is_overwrite = params[\"is_overwrite\"]\n",
    "        self.org_columns = []\n",
    "        self.logger = logger\n",
    "\n",
    "        self.input_dir = os.path.join(os.path.dirname(\"__file__\"), \"../input\")\n",
    "        self.df_path = Path(self.input_dir) / f\"{self.datatype}.csv\"\n",
    "\n",
    "        self.save_dir = os.path.join(os.path.dirname(\"__file__\"), f\"../feature\")\n",
    "        self.save_type_dir = Path(self.save_dir) / f\"{self.datatype}\"\n",
    "        self.save_path = Path(self.save_type_dir) / f\"{self.name}.pkl\"\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        os.makedirs(self.save_type_dir, exist_ok=True)\n",
    "\n",
    "    def feature_extract(self, org_train, org_test):\n",
    "        if self.check_feature_exec():\n",
    "            with timer(f\"FE: {self.name}\", self.logger):\n",
    "                a = self.calc_feature(org_train, org_test)\n",
    "            return a\n",
    "\n",
    "    @abstractmethod\n",
    "    def calc_feature(self):\n",
    "        \"\"\"calc and save features\n",
    "        Return: feature_df\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def format_and_save_feats(self, feat_df):\n",
    "        \"\"\"保存するカラムなど特徴量の形式を指定する\n",
    "        \"\"\"\n",
    "        feat_cols = [c for c in list(feat_df.columns) if c not in self.org_columns]\n",
    "        pickle_dump(feat_df[feat_cols], self.save_path)\n",
    "\n",
    "        del feat_df\n",
    "        gc.collect()\n",
    "\n",
    "    def check_feature_exec(self):\n",
    "        \"\"\"\n",
    "        すでに対象の特徴が存在するかどうかをcheckする\n",
    "        Returns: bool (Falseなら特徴作成しない)\n",
    "\n",
    "        \"\"\"\n",
    "        path = self.save_path\n",
    "\n",
    "        if self.is_overwrite:\n",
    "            print(f\"overwrite features : {self.name}\")\n",
    "            return True\n",
    "        else:\n",
    "            if os.path.exists(path) is False:\n",
    "                print(f\"creates new file : {self.name}\")\n",
    "                return True\n",
    "\n",
    "        print(f\"file exists : {self.name}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class LightGBM():\n",
    "    def __init__(self, param):\n",
    "\n",
    "        self.predict_type = param[\"predict_type\"] # classifier, regressor\n",
    "        self.train_params = param[\"train_params\"]\n",
    "        self.train_cols = param[\"train_cols\"]\n",
    "        self.cat_cols = param[\"cat_cols\"]\n",
    "        self.target = param[\"target\"]\n",
    "        self.is_debug = param[\"is_debug\"]\n",
    "\n",
    "    def train(self, train, valid, logger):\n",
    "        if type(train) != pd.DataFrame or type(valid) != pd.DataFrame:\n",
    "            raise ValueError('Parameter train and valid must be pandas.DataFrame')\n",
    "\n",
    "        if list(train.columns) != list(valid.columns):\n",
    "            raise ValueError('Train and valid must have a same column list')\n",
    "\n",
    "        trn_x, trn_y = train[self.train_cols], train[self.target]\n",
    "        val_x, val_y = valid[self.train_cols], valid[self.target]\n",
    "        callbacks = [log_evaluation(logger, period=500)]\n",
    "        \n",
    "        if self.predict_type == \"binary_classifier\":\n",
    "            clf = lgb.LGBMClassifier(**self.train_params)\n",
    "            clf.fit(\n",
    "                trn_x, trn_y,\n",
    "                eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "                verbose=500,\n",
    "                early_stopping_rounds=500,\n",
    "                callbacks=callbacks,\n",
    "                categorical_feature = self.cat_cols,\n",
    "            )\n",
    "            oof = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "            \n",
    "        elif self.predict_type == \"multi_classifier\":\n",
    "            clf = lgb.LGBMClassifier(**self.train_params)\n",
    "            clf.fit(\n",
    "                trn_x, trn_y,\n",
    "                eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "                verbose=500,\n",
    "                early_stopping_rounds=500,\n",
    "                callbacks=callbacks,\n",
    "                categorical_feature = self.cat_cols,\n",
    "                eval_metric=eval_qwk_lgb\n",
    "            )\n",
    "            oof = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "\n",
    "        elif self.predict_type == \"regressor\":\n",
    "            clf = lgb.LGBMRegressor(**self.train_params)\n",
    "            clf.fit(\n",
    "                trn_x, trn_y,\n",
    "                eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "                verbose=500,\n",
    "                early_stopping_rounds=500,\n",
    "                callbacks=callbacks,\n",
    "                categorical_feature=self.cat_cols,\n",
    "            )\n",
    "\n",
    "            oof = clf.predict(val_x, num_iteration=clf.best_iteration_)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"unknown prediction type !!\")\n",
    "\n",
    "        self.clf = clf\n",
    "\n",
    "        # feature importance\n",
    "        feature_importance_df = pd.DataFrame()\n",
    "        feature_importance_df[\"feature\"] = self.train_cols\n",
    "        feature_importance_df[\"importance\"] = self.clf.feature_importances_\n",
    "\n",
    "        return clf, oof, feature_importance_df\n",
    "\n",
    "    def predict(self, test, logger):\n",
    "        if self.predict_type == \"classifier\":\n",
    "            prediction = self.clf.predict_proba(test[self.train_cols],\n",
    "                                                num_iteration=self.clf.best_iteration_)[:, 1]\n",
    "        elif self.predict_type == \"multi_classifier\":\n",
    "            prediction = self.clf.predict_proba(test[self.train_cols],\n",
    "                                                num_iteration=self.clf.best_iteration_)\n",
    "        elif self.predict_type == \"regressor\":\n",
    "            prediction = self.clf.predict(test[self.train_cols],\n",
    "                                          num_iteration=self.clf.best_iteration_)\n",
    "        else:\n",
    "            raise ValueError(\"unknown prediction type !!\")\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def save_model(self, save_dir):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QWK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def qwk(y_true, y_pred, max_rat= 3):\n",
    "    y_true_ = np.asarray(y_true, dtype=int)\n",
    "    y_pred_ = np.asarray(y_pred, dtype=int)\n",
    "\n",
    "    hist1 = np.zeros((max_rat + 1, ))\n",
    "    hist2 = np.zeros((max_rat + 1, ))\n",
    "\n",
    "    numerator = 0\n",
    "    for k in range(y_true_.shape[0]):\n",
    "        i, j = y_true_[k], y_pred_[k]\n",
    "        hist1[i] += 1\n",
    "        hist2[j] += 1\n",
    "        numerator += (i - j) * (i - j)\n",
    "\n",
    "    denominator = 0\n",
    "    for i in range(max_rat + 1):\n",
    "        for j in range(max_rat + 1):\n",
    "            denominator += hist1[i] * hist2[j] * (i - j) * (i - j)\n",
    "\n",
    "    denominator /= y_true_.shape[0]\n",
    "    return 1 - numerator / denominator\n",
    "\n",
    "def eval_qwk_lgb(y_true: Union[np.ndarray, list],\n",
    "                           y_pred: Union[np.ndarray, list],) -> Tuple[str, float, bool]:\n",
    "    y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)\n",
    "    return \"qwk\", qwk(y_true, y_pred), True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define validation class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validation():\n",
    "    def __init__(self, validation_param, exp_conf, train, test, logger):\n",
    "        self.model_name = validation_param[\"model_name\"]\n",
    "        self.train_small_dataset = exp_conf[\"train_small_dataset\"]\n",
    "#         self.common_conf = common_conf\n",
    "        self.logger = logger\n",
    "        self.exp_conf = exp_conf\n",
    "        self.train = self.fix_train_size(train)\n",
    "        self.test = test\n",
    "        self.feature_importance = []\n",
    "\n",
    "        self.logging_valid_parameters()\n",
    "\n",
    "    def logging_valid_parameters(self):\n",
    "#         self.logger.log(logging.DEBUG, self.common_conf)\n",
    "        self.logger.log(logging.DEBUG, \"[use_feature] \" + \"-\" * 50)\n",
    "        self.logger.log(logging.DEBUG, self.exp_conf[\"use_feature\"])\n",
    "        self.logger.log(logging.DEBUG, \"[train_params] \" + \"-\"*50)\n",
    "        self.logger.log(logging.DEBUG, self.exp_conf[\"train_params\"])\n",
    "\n",
    "    def fix_train_size(self, train):\n",
    "        if self.train_small_dataset:\n",
    "            self.logger.log(logging.DEBUG, \"Down-sampling train data.\")\n",
    "            self.logger.log(logging.DEBUG, f\"Org-shape:{train.shape}\")\n",
    "            p = 0.15  # 学習に使用する割合\n",
    "            np.random.seed(773)\n",
    "            int_p = int(len(train.index.values) * p)\n",
    "            sample_index = np.random.choice(train.index.values, int_p, replace=False)  # 重複なし\n",
    "\n",
    "            train = train.loc[train.index.isin(sample_index)].reset_index(drop=True)\n",
    "            self.logger.log(logging.DEBUG, f\"sampled train-shape:{train.shape}\")\n",
    "            return train\n",
    "\n",
    "        return train\n",
    "\n",
    "    def generate_model(self, model_conf):\n",
    "        if self.model_name == \"LGBM\":\n",
    "            model = LightGBM(model_conf)\n",
    "        else:\n",
    "            raise ValueError(\"permitted models are [LGBM, ..., ]\")\n",
    "        return model\n",
    "\n",
    "    def do_valid_kfold(self, model_conf, n_splits=5):\n",
    "        sp = Splitter()\n",
    "        target = model_conf[\"target\"]\n",
    "        split_x = self.train[\"installation_id\"]\n",
    "        split_y = self.train[target]\n",
    "        seed = 773\n",
    "        sp.get_kfold_idx(split_x, split_y, seed, n_cv=n_splits, stratified=False, group=True, pref=self.exp_conf[\"exp_name\"])\n",
    "\n",
    "        oof: ndarray = np.zeros((self.train.shape[0]))\n",
    "        prediction = np.zeros((self.test.shape[0]))\n",
    "\n",
    "        clf_list = []\n",
    "\n",
    "        self.logger.log(logging.DEBUG, \"[train cols] \" + \"-\"*50)\n",
    "        self.logger.log(logging.DEBUG, model_conf[\"train_cols\"])\n",
    "        self.validation_scores = []\n",
    "\n",
    "        for i, (trn_idx, val_idx) in enumerate(sp.idx_list):\n",
    "            self.logger.log(logging.DEBUG, \"-\" * 60)\n",
    "            self.logger.log(logging.DEBUG, f\"start training: {i}\")\n",
    "\n",
    "            with timer(f\"fold {i}\", self.logger):\n",
    "                train_df, valid_df = self.train.loc[trn_idx], self.train.loc[val_idx]\n",
    "                model = self.generate_model(model_conf)\n",
    "                clf, fold_oof, feature_importance_df = model.train(train_df, valid_df, self.logger)\n",
    "#                 fold_oof_class = fold_oof.argmax(axis = 1)\n",
    "                \n",
    "                fold_prediction = model.predict(self.test, self.logger)\n",
    "#                 fold_val_score = get_val_score(valid_df[target], fold_oof_class, \"QWK\")\n",
    "\n",
    "                # calc validation score using best iteration\n",
    "#                 self.validation_scores.append(fold_val_score)\n",
    "#                 self.logger.log(logging.DEBUG, f\"fold_val_score: {fold_val_score:,.5f}\")\n",
    "                \n",
    "                clf_list.append(clf)\n",
    "                oof[val_idx] = fold_oof\n",
    "\n",
    "                prediction += fold_prediction / n_splits\n",
    "\n",
    "                feature_importance_df[\"fold\"] = i\n",
    "                self.feature_importance.append(feature_importance_df)\n",
    "\n",
    "#         self.logger.log(logging.DEBUG,\n",
    "#                         f\"Total Validation Score: {sum(self.validation_scores) / len(self.validation_scores):,.5f}\")\n",
    "\n",
    "        self.feature_importance = pd.concat(self.feature_importance, axis=0)\n",
    "\n",
    "        return clf_list, oof, prediction, self.feature_importance\n",
    "\n",
    "    def do_adversarial_valid_kfold(self, model_conf, n_splits=2):\n",
    "        sp = Splitter()\n",
    "        target = \"is_test\"\n",
    "        split_x = self.train[\"installation_id\"]\n",
    "        split_y = self.train[target]\n",
    "        seed = 773\n",
    "        sp.get_kfold_idx(split_x, split_y, seed, n_cv=n_splits, stratified=True, pref=\"adv\")\n",
    "\n",
    "        target_length = 1\n",
    "        oof: ndarray = np.zeros(self.train.shape[0])\n",
    "        prediction = np.zeros(self.test.shape[0])\n",
    "\n",
    "        clf_list = []\n",
    "\n",
    "        self.logger.log(logging.DEBUG, \"[train cols] \" + \"-\"*50)\n",
    "        self.logger.log(logging.DEBUG, model_conf[\"train_cols\"])\n",
    "        self.validation_scores = []\n",
    "\n",
    "        for i, (trn_idx, val_idx) in enumerate(sp.idx_list):\n",
    "            self.logger.log(logging.DEBUG, \"-\" * 60)\n",
    "            self.logger.log(logging.DEBUG, f\"start training: {i}\")\n",
    "\n",
    "            with timer(f\"fold {i}\", self.logger):\n",
    "                train_df, valid_df = self.train.loc[trn_idx], self.train.loc[val_idx]\n",
    "                model = self.generate_model(model_conf)\n",
    "                clf, fold_oof, feature_importance_df = model.train(train_df, valid_df, self.logger)\n",
    "\n",
    "                # calc validation score using clf.best_iteration_\n",
    "                fold_val_score = get_val_score(valid_df[target], fold_oof)\n",
    "                self.validation_scores.append(fold_val_score)\n",
    "                self.logger.log(logging.DEBUG, f\"fold_val_score: {fold_val_score:,.5f}\")\n",
    "\n",
    "                clf_list.append(clf)\n",
    "                oof[val_idx] = fold_oof\n",
    "\n",
    "                feature_importance_df[\"fold\"] = i\n",
    "                self.feature_importance.append(feature_importance_df)\n",
    "\n",
    "        self.logger.log(logging.DEBUG,\n",
    "                        f\"Total Validation Score: {sum(self.validation_scores) / len(self.validation_scores):,.5f}\")\n",
    "\n",
    "        oof = np.expm1(oof)\n",
    "        self.train[\"pred_y\"] = oof\n",
    "        self.feature_importance = pd.concat(self.feature_importance, axis=0)\n",
    "\n",
    "        return clf_list, oof, prediction, self.feature_importance\n",
    "\n",
    "\n",
    "class Splitter():\n",
    "    def __init__(self):\n",
    "        self.save_dir = os.path.join(os.path.dirname(\"__file__\"), \"../data/valid_idx\")\n",
    "        self.idx_list = []\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def get_kfold_idx(self, split_x, split_y, seed, n_cv=5, stratified=True, group=False, pref=\"\"):\n",
    "        if group is False:\n",
    "            if stratified:\n",
    "                self.folds = StratifiedKFold(n_splits=n_cv, shuffle=False, random_state=seed)\n",
    "            else:\n",
    "                self.folds = KFold(n_splits=n_cv, shuffle=True, random_state=seed)\n",
    "\n",
    "            for i, (trn_, val_) in enumerate(self.folds.split(split_x, split_y)):\n",
    "                self.idx_list.append([trn_, val_])\n",
    "                idx_trn_path = os.path.join(self.save_dir, \"trn_idx_{}{}_{}.npy\".format(pref, i, seed))\n",
    "                idx_val_path = os.path.join(self.save_dir, \"val_idx_{}{}_{}.npy\".format(pref, i, seed))\n",
    "                np.save(idx_trn_path, trn_)\n",
    "                np.save(idx_val_path, val_)\n",
    "\n",
    "            return self.idx_list\n",
    "    \n",
    "        else:\n",
    "            groups = split_x\n",
    "            self.folds = GroupKFold(n_splits=n_cv)\n",
    "            for i, (trn_, val_) in enumerate(self.folds.split(split_x, split_y, groups)):\n",
    "                self.idx_list.append([trn_, val_])\n",
    "                idx_trn_path = os.path.join(self.save_dir, \"trn_idx_{}{}_{}.npy\".format(pref, i, seed))\n",
    "                idx_val_path = os.path.join(self.save_dir, \"val_idx_{}{}_{}.npy\".format(pref, i, seed))\n",
    "                np.save(idx_trn_path, trn_)\n",
    "                np.save(idx_val_path, val_)\n",
    "\n",
    "            return self.idx_list                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sample_feature(Features):\n",
    "    \"\"\"sample feature\n",
    "    \"\"\"\n",
    "    def __int__(self, params, logger):\n",
    "        super().__init__(params, logger)\n",
    "\n",
    "    def calc_feature(self, org_train, org_test):\n",
    "        \n",
    "        if self.datatype == \"train\":\n",
    "            df = org_train\n",
    "        else:\n",
    "            df = org_test\n",
    "            \n",
    "        feat_df = df.groupby(\"installation_id\")[\"title\"].count().reset_index()\n",
    "        self.format_and_save_feats(feat_df)\n",
    "        \n",
    "        return feat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## base kernel features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class KernelBasics(Features):\n",
    "    \"\"\"kernel features revised\n",
    "    \"\"\"\n",
    "    def __init__(self, train_labels, params, logger=None):        \n",
    "        super().__init__(params, logger=logger)\n",
    "        self.train_labels = train_labels        \n",
    "        \n",
    "    def mapping_codes(self, org_train, org_test):\n",
    "        self.all_activities = set(org_train[\"title\"].unique()).union(\n",
    "            set(org_test[\"title\"].unique()))\n",
    "        self.all_event_codes = set(org_train[\"event_code\"].unique()).union(\n",
    "            org_test[\"event_code\"].unique())\n",
    "        \n",
    "        # convert activities <=> int\n",
    "        self.activities_map = dict(\n",
    "            zip(self.all_activities, np.arange(len(self.all_activities)))) # activity title => int \n",
    "        self.inverse_activities_map = dict(\n",
    "            zip(np.arange(len(self.all_activities)), self.all_activities)) # int => activity title \n",
    "        \n",
    "        # convert win_code <=> int \n",
    "        win_code = dict(\n",
    "            zip(activities_map.values(),\n",
    "                (4100 * np.ones(len(activities_map))).astype(int)))\n",
    "        win_code[activities_map[\"Bird Measurer (Assessment)\"]] = 4110\n",
    "        \n",
    "        self.win_code = win_code\n",
    "        \n",
    "    def calc_feature(self, org_train, org_test):                \n",
    "        if self.datatype == \"train\":\n",
    "            df = org_train\n",
    "            assess_user = df.loc[df.type == \"Assessment\"].installation_id.unique()\n",
    "            df = df.loc[df.installation_id.isin(assess_user)]\n",
    "        else:\n",
    "            # 直前までのnum_correct/incorrectを取得する\n",
    "            org_test.loc[(org_test.event_code.isin([4100, 4110])) & (org_test[\"event_data\"].str.contains(\"true\")), 'num_correct'] = 1\n",
    "            org_test.loc[(org_test.event_code.isin([4100, 4110])) & (org_test[\"event_data\"].str.contains(\"false\")), 'num_incorrect'] = 1    \n",
    "            df = org_test\n",
    "        \n",
    "        ret = applyParallel(df.groupby(\"installation_id\"), self.ins_id_sessions)\n",
    "        ret_col = [c for c in list(ret.columns) if c not in [\"accuracy\",\"accuracy_group\",\"cum_accuracy\",\n",
    "                                                             \"game_session\",\"installation_id\",\"title\",\n",
    "                                                             \"type\"\n",
    "                                                            ]]\n",
    "        \n",
    "        ret[ret_col] = ret[ret_col].fillna(0).astype(\"int32\")\n",
    "        self.format_and_save_feats(ret)\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "    def ins_id_sessions(self, df):\n",
    "        \"\"\"session当該session直前までのactivityを示す\n",
    "        Args:\n",
    "            df: df grouped by installation_id \n",
    "        \"\"\"\n",
    "        # initialize user activity \n",
    "        # 1. time spent \n",
    "        # 2. event count \n",
    "        # 3. session count \n",
    "        \n",
    "        # sessionごとのplaytimeを算出\n",
    "        df[\"gs_max_time\"] = df.groupby(\"game_session\")[\"timestamp\"].transform(\"max\") # gs_max_timeでsortする必要がある\n",
    "\n",
    "        pv = pd.pivot_table(df, index=[\"gs_max_time\", \"game_session\", \"type\"],  \n",
    "                            columns=\"title\", \n",
    "                            values=\"game_time\", \n",
    "                            aggfunc=\"max\").fillna(0)\n",
    "        \n",
    "        # 時刻順に並ぶことを保証する\n",
    "        pv.sort_values(\"gs_max_time\", ascending=True, inplace=True)\n",
    "        pv.reset_index(inplace=True)\n",
    "        \n",
    "        cum_cols = [c for c in list(pv.columns) if c not in [\"type\", \"game_session\", \"gs_max_time\"]]\n",
    "        pv[cum_cols] = (pv[cum_cols].cumsum() // 1000).astype(\"int32\")\n",
    "        pv[cum_cols] = pv[cum_cols].shift(1) # 直前までのplaytimeを取得する\n",
    "        \n",
    "        ins_id = df.installation_id.values[0]\n",
    "        \n",
    "        # assessmentのrowに限定して抽出する\n",
    "        if self.datatype == \"train\":\n",
    "            # 正解ラベル/num_corrects を得るためtrain labelsとmerge\n",
    "            pv = pd.merge(pv, self.train_labels[self.train_labels.installation_id == ins_id], how=\"inner\", on=\"game_session\")\n",
    "        else:\n",
    "            # calc num corrects \n",
    "            num_c_df = df.loc[df.type == \"Assessment\"].groupby([\"installation_id\",\"game_session\"])[[\"num_correct\", \"num_incorrect\"]].sum().fillna(0).reset_index()\n",
    "            pv = pd.merge(pv, num_c_df, how=\"left\", on=\"game_session\")\n",
    "            \n",
    "        gc.collect()\n",
    "        \n",
    "        # 直前までの正解状況を集計\n",
    "        pv[\"prev_num_corrects\"] = pv[\"num_correct\"].shift(1).fillna(0)\n",
    "        pv[\"prev_cumnum_c\"] = pv[\"prev_num_corrects\"].cumsum()\n",
    "        pv[\"prev_num_incorrects\"] = pv[\"num_incorrect\"].shift(1).fillna(0)\n",
    "        pv[\"prev_cumnum_inc\"] = pv[\"prev_num_incorrects\"].cumsum()\n",
    "\n",
    "        pv[\"cum_accuracy\"] = (pv[\"prev_cumnum_c\"] / \n",
    "                                     (pv[\"prev_cumnum_c\"] + pv[\"prev_cumnum_inc\"])).fillna(0)\n",
    "        \n",
    "        del pv[\"num_correct\"], pv[\"num_incorrect\"], pv[\"gs_max_time\"], pv[\"prev_num_corrects\"], pv[\"prev_num_incorrects\"]\n",
    "        gc.collect()\n",
    "        \n",
    "        if self.datatype==\"test\":\n",
    "            pv = pd.DataFrame([pv.iloc[-1, :]])\n",
    "\n",
    "        return pv\n",
    "    \n",
    "    \n",
    "class KernelBasics2(Features):\n",
    "    \"\"\"kernel features revised\n",
    "    \"\"\"\n",
    "    def __init__(self, train_labels, params, logger=None):        \n",
    "        super().__init__(params, logger=logger)\n",
    "        self.train_labels = train_labels        \n",
    "        \n",
    "    def mapping_codes(self, org_train, org_test):\n",
    "        self.all_activities = set(org_train[\"title\"].unique()).union(\n",
    "            set(org_test[\"title\"].unique()))\n",
    "        self.all_event_codes = set(org_train[\"event_code\"].unique()).union(\n",
    "            org_test[\"event_code\"].unique())\n",
    "        \n",
    "        # convert activities <=> int\n",
    "        self.activities_map = dict(\n",
    "            zip(self.all_activities, np.arange(len(self.all_activities)))) # activity title => int \n",
    "        self.inverse_activities_map = dict(\n",
    "            zip(np.arange(len(self.all_activities)), self.all_activities)) # int => activity title \n",
    "        \n",
    "        # convert win_code <=> int \n",
    "        win_code = dict(\n",
    "            zip(activities_map.values(),\n",
    "                (4100 * np.ones(len(activities_map))).astype(int)))\n",
    "        win_code[activities_map[\"Bird Measurer (Assessment)\"]] = 4110\n",
    "        \n",
    "        self.win_code = win_code\n",
    "        \n",
    "    def calc_feature(self, org_train, org_test):                \n",
    "        if self.datatype == \"train\":\n",
    "            df = org_train\n",
    "            df = df.loc[df.installation_id.isin(self.train_labels.installation_id.unique())]\n",
    "        else:\n",
    "            # 直前までのnum_correct/incorrectを取得する\n",
    "            df = org_test\n",
    "            c_ass_idx = ((df.event_code == 4100) \n",
    "                          & (df.title != \"Bird Measurer (Assessment)\") \n",
    "                          & (df[\"event_data\"].str.contains(\"true\"))) | \\\n",
    "                         ((df.event_code == 4110) \n",
    "                          & (df.title == \"Bird Measurer (Assessment)\") \n",
    "                          & (df[\"event_data\"].str.contains(\"true\")))\n",
    "\n",
    "            inc_ass_idx = ((df.event_code == 4100) \n",
    "                          & (df.title != \"Bird Measurer (Assessment)\") \n",
    "                          & (df[\"event_data\"].str.contains(\"false\"))) | \\\n",
    "                         ((df.event_code == 4110) \n",
    "                          & (df.title == \"Bird Measurer (Assessment)\") \n",
    "                          & (df[\"event_data\"].str.contains(\"false\")))\n",
    "            \n",
    "            df.loc[c_ass_idx, 'num_correct'] = 1\n",
    "            df.loc[inc_ass_idx, 'num_incorrect'] = 1    \n",
    "        \n",
    "        ret = applyParallel(df.groupby(\"installation_id\"), self.ins_id_sessions)\n",
    "        ret_col = [c for c in list(ret.columns) if c not in [\"accuracy\",\"accuracy_group\",\"cum_accuracy\",\n",
    "                                                             \"game_session\",\"installation_id\",\"title\",\n",
    "                                                             \"type\"\n",
    "                                                            ]]\n",
    "        \n",
    "        ret[ret_col] = ret[ret_col].fillna(0).astype(\"int32\")\n",
    "        self.format_and_save_feats(ret)\n",
    "        \n",
    "        use_cols = [c for c in list(ret.columns) if \"Assessment\" not in c]\n",
    "        \n",
    "        return ret[use_cols]\n",
    "    \n",
    "    def ins_id_sessions(self, df):\n",
    "        \"\"\"session当該session直前までのactivityを示す\n",
    "        Args:\n",
    "            df: df grouped by installation_id \n",
    "        \"\"\"\n",
    "        # initialize user activity \n",
    "        # 1. time spent \n",
    "        # 2. event count \n",
    "        # 3. session count \n",
    "        \n",
    "        # sessionごとのplaytimeを算出\n",
    "        df[\"gs_max_time\"] = df.groupby(\"game_session\")[\"timestamp\"].transform(\"max\") # gs_max_timeでsortする必要がある\n",
    "\n",
    "        pv = pd.pivot_table(df, index=[\"gs_max_time\", \"game_session\", \"type\"],  \n",
    "                            columns=\"title\", \n",
    "                            values=\"game_time\", \n",
    "                            aggfunc=\"max\").fillna(0)\n",
    "        \n",
    "        # 時刻順に並ぶことを保証する\n",
    "        pv.sort_values(\"gs_max_time\", ascending=True, inplace=True)\n",
    "        pv.reset_index(inplace=True)\n",
    "        \n",
    "        cum_cols = [c for c in list(pv.columns) if c not in [\"type\", \"game_session\", \"gs_max_time\"]]\n",
    "        pv[cum_cols] = (pv[cum_cols].cumsum() // 1000).astype(\"int32\")\n",
    "        pv[cum_cols] = pv[cum_cols].shift(1) # 直前までのplaytimeを取得する\n",
    "        \n",
    "        ins_id = df.installation_id.values[0]\n",
    "        \n",
    "        # assessmentのrowに限定して抽出する\n",
    "        if self.datatype == \"train\":\n",
    "            # 正解ラベル/num_corrects を得るためtrain labelsとmerge\n",
    "            pv = pd.merge(pv, self.train_labels[self.train_labels.installation_id == ins_id], how=\"inner\", on=\"game_session\")\n",
    "        else:\n",
    "            # calc num corrects \n",
    "            num_c_df = df.loc[df.type == \"Assessment\"].groupby([\"installation_id\",\"game_session\"])[[\"num_correct\", \"num_incorrect\"]].sum().fillna(0).reset_index()\n",
    "            pv = pd.merge(pv, num_c_df, how=\"left\", on=\"game_session\")\n",
    "            \n",
    "        gc.collect()\n",
    "        \n",
    "        # 直前までの正解状況を集計\n",
    "        pv[\"prev_num_corrects\"] = pv[\"num_correct\"].shift(1).fillna(0)\n",
    "        pv[\"prev_cumnum_c\"] = pv[\"prev_num_corrects\"].cumsum()\n",
    "        pv[\"prev_num_incorrects\"] = pv[\"num_incorrect\"].shift(1).fillna(0)\n",
    "        pv[\"prev_cumnum_inc\"] = pv[\"prev_num_incorrects\"].cumsum()\n",
    "\n",
    "        pv[\"cum_accuracy\"] = (pv[\"prev_cumnum_c\"] / \n",
    "                                     (pv[\"prev_cumnum_c\"] + pv[\"prev_cumnum_inc\"])).fillna(0)\n",
    "        \n",
    "        del pv[\"num_correct\"], pv[\"num_incorrect\"]\n",
    "        gc.collect()\n",
    "        \n",
    "        pv = self.get_acc_group(pv)        \n",
    "        del pv[\"gs_max_time\"]\n",
    "        \n",
    "        if self.datatype==\"test\":\n",
    "            pv = pd.DataFrame([pv.iloc[-1, :]])\n",
    "\n",
    "        return pv\n",
    "    \n",
    "    def get_acc_group(self, pv):\n",
    "        def calc_accuracy_group(row):\n",
    "            if row[\"prev_num_incorrects\"] + row[\"prev_num_corrects\"] > 0:\n",
    "                acc = row[\"prev_num_corrects\"] / (row[\"prev_num_incorrects\"] + row[\"prev_num_corrects\"])\n",
    "                if acc == 0:\n",
    "                    return 0\n",
    "                elif acc == 1:\n",
    "                    return 3\n",
    "                elif acc == 0.5:\n",
    "                    return 2\n",
    "                else:\n",
    "                    return 1\n",
    "            else:\n",
    "                return -99\n",
    "\n",
    "        pv[\"acc_group\"] = pv.apply(calc_accuracy_group, axis=1)\n",
    "        acc_pv = pd.pivot_table(pv[[\"gs_max_time\", \"installation_id\", \"game_session\", \"acc_group\"]], index=[\"gs_max_time\", \"game_session\"], columns=\"acc_group\", values=\"installation_id\", aggfunc=\"count\").reset_index().fillna(0)\n",
    "        \n",
    "        del pv[\"acc_group\"]\n",
    "        \n",
    "        acc_columns = {}\n",
    "        for col in acc_pv.columns:\n",
    "            if col in [-99, 0, 1, 2, 3]:\n",
    "                acc_columns[col] = \"prev_acc_gr_\" + str(col)\n",
    "                acc_pv[f\"accum_acc_gr_{col}\" ] = acc_pv[col].cumsum()\n",
    "\n",
    "        acc_pv.rename(columns=acc_columns, inplace=True)\n",
    "        del acc_pv[\"gs_max_time\"]\n",
    "        pv = pd.merge(pv, acc_pv, on=\"game_session\", how=\"left\")    \n",
    "\n",
    "        return pv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EventCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class EventCount(Features):\n",
    "    \"\"\"kernel features revised\n",
    "    \"\"\"\n",
    "    def __init__(self, train_labels, params, logger=None):        \n",
    "        super().__init__(params, logger=logger)\n",
    "        self.train_labels = train_labels        \n",
    "        \n",
    "    def calc_feature(self, org_train, org_test):                \n",
    "        if self.datatype == \"train\":\n",
    "            df = org_train\n",
    "            assess_user = df.loc[df.type == \"Assessment\"].installation_id.unique()\n",
    "            df = df.loc[df.installation_id.isin(assess_user)]\n",
    "        else:\n",
    "            # 直前までのnum_correct/incorrectを取得する\n",
    "            org_test.loc[(org_test.event_code.isin([4100, 4110])) & (org_test[\"event_data\"].str.contains(\"true\")), 'num_correct'] = 1\n",
    "            org_test.loc[(org_test.event_code.isin([4100, 4110])) & (org_test[\"event_data\"].str.contains(\"false\")), 'num_incorrect'] = 1    \n",
    "            df = org_test\n",
    "        \n",
    "        ret = applyParallel(df.groupby(\"installation_id\"), self.ins_id_sessions)\n",
    "        ret_col = [c for c in list(ret.columns) if c not in [\"accuracy\",\"accuracy_group\",\"cum_accuracy\",\n",
    "                                                             \"game_session\",\"installation_id\",\"title\",\n",
    "                                                             \"type\"\n",
    "                                                            ]]\n",
    "        ret[ret_col] = ret[ret_col].fillna(0).astype(\"int32\")\n",
    "#         self.format_and_save_feats(ret)\n",
    "\n",
    "        use_cols = [c for c in list(ret.columns) if c not in [\"accuracy\",\"accuracy_group\",\"cum_accuracy\",\"title\",\n",
    "                                                             \"type\", \"event_code\", \"gs_max_time\"\n",
    "                                                            ]]\n",
    "        \n",
    "        self.format_and_save_feats(ret[use_cols])\n",
    "        \n",
    "        return ret[use_cols]\n",
    "    \n",
    "    def ins_id_sessions(self, df):\n",
    "        \"\"\"session当該session直前までのactivityを示す\n",
    "        Args:\n",
    "            df: df grouped by installation_id \n",
    "        \"\"\"\n",
    "        df[\"gs_max_time\"] = df.groupby(\"game_session\")[\"timestamp\"].transform(\"max\") # gs_max_timeでsortする必要がある\n",
    "\n",
    "        pv = pd.pivot_table(df, index=[\"installation_id\", \"gs_max_time\", \"game_session\", \"type\"],  \n",
    "                            columns=\"event_code\", \n",
    "                            values=\"timestamp\", \n",
    "                            aggfunc=\"count\").fillna(0)\n",
    "\n",
    "        # 時刻順に並ぶことを保証する\n",
    "        pv.sort_values(\"gs_max_time\", ascending=True, inplace=True)\n",
    "        pv.reset_index(inplace=True)\n",
    "\n",
    "        cum_cols = [c for c in list(pv.columns) if c not in [\"installation_id\", \"type\", \"game_session\", \"gs_max_time\"]]\n",
    "        pv[cum_cols] = pv[cum_cols].cumsum().shift(1).fillna(0).astype(\"int32\")\n",
    "        \n",
    "        pv = pv.loc[pv[\"type\"] == \"Assessment\"] # assessment だけとればOK\n",
    "        \n",
    "        rename_dict = {}\n",
    "        for c in cum_cols:\n",
    "            rename_dict[c] = \"ev_cnt\" + str(c)     \n",
    "        pv.rename(columns=rename_dict, inplace=True)\n",
    "        pv.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        del pv[\"gs_max_time\"], pv[\"type\"]\n",
    "        gc.collect()\n",
    "        \n",
    "        if self.datatype==\"test\":\n",
    "            pv = pd.DataFrame([pv.iloc[-1, :]])\n",
    "\n",
    "        return pv\n",
    "class EventCount2(Features):\n",
    "    \"\"\"kernel features revised\n",
    "    \"\"\"\n",
    "    def __init__(self, train_labels, params, logger=None):        \n",
    "        super().__init__(params, logger=logger)\n",
    "        self.train_labels = train_labels        \n",
    "        \n",
    "    def calc_feature(self, org_train, org_test):                \n",
    "        if self.datatype == \"train\":\n",
    "            df = org_train\n",
    "            assess_user = df.loc[df.type == \"Assessment\"].installation_id.unique()\n",
    "            df = df.loc[df.installation_id.isin(assess_user)]\n",
    "        else:\n",
    "            # 直前までのnum_correct/incorrectを取得する\n",
    "            org_test.loc[(org_test.event_code.isin([4100, 4110])) & (org_test[\"event_data\"].str.contains(\"true\")), 'num_correct'] = 1\n",
    "            org_test.loc[(org_test.event_code.isin([4100, 4110])) & (org_test[\"event_data\"].str.contains(\"false\")), 'num_incorrect'] = 1    \n",
    "            df = org_test\n",
    "        \n",
    "        ret = applyParallel(df.groupby(\"installation_id\"), self.ins_id_sessions)\n",
    "        ret_col = [c for c in list(ret.columns) if c not in [\"accuracy\",\"accuracy_group\",\"cum_accuracy\",\n",
    "                                                             \"game_session\",\"installation_id\",\"title\",\n",
    "                                                             \"type\"\n",
    "                                                            ]]\n",
    "        ret[ret_col] = ret[ret_col].fillna(0).astype(\"int32\")\n",
    "#         self.format_and_save_feats(ret)\n",
    "\n",
    "        use_cols = [c for c in list(ret.columns) if c not in [\"accuracy\",\"accuracy_group\",\"cum_accuracy\",\"title\",\n",
    "                                                             \"type\", \"event_code\", \"gs_max_time\"\n",
    "                                                            ]]\n",
    "        \n",
    "        self.format_and_save_feats(ret[use_cols])\n",
    "        \n",
    "        return ret[use_cols]\n",
    "    \n",
    "    def ins_id_sessions(self, df):\n",
    "        \"\"\"session当該session直前までのactivityを示す\n",
    "        Args:\n",
    "            df: df grouped by installation_id \n",
    "        \"\"\"\n",
    "        df[\"gs_max_time\"] = df.groupby(\"game_session\")[\"timestamp\"].transform(\"max\") # gs_max_timeでsortする必要がある\n",
    "        \n",
    "        pv = pd.pivot_table(df, index=[\"installation_id\", \"gs_max_time\", \"game_session\", \"type\"],  \n",
    "                            columns=\"event_id\", \n",
    "                            values=\"timestamp\", \n",
    "                            aggfunc=\"count\").fillna(0)\n",
    "\n",
    "        # 時刻順に並ぶことを保証する\n",
    "        pv.sort_values(\"gs_max_time\", ascending=True, inplace=True)\n",
    "        pv.reset_index(inplace=True)\n",
    "\n",
    "        cum_cols = [c for c in list(pv.columns) if c not in [\"installation_id\", \"type\", \"game_session\", \"gs_max_time\"]]\n",
    "        pv[cum_cols] = pv[cum_cols].cumsum().shift(1).fillna(0).astype(\"int32\")\n",
    "        \n",
    "        pv = pv.loc[pv[\"type\"] == \"Assessment\"] # assessment だけとればOK\n",
    "        \n",
    "        rename_dict = {}\n",
    "        for c in cum_cols:\n",
    "            rename_dict[c] = \"ev_cnt\" + str(c)     \n",
    "        pv.rename(columns=rename_dict, inplace=True)\n",
    "        pv.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        del pv[\"gs_max_time\"], pv[\"type\"]\n",
    "        gc.collect()\n",
    "        \n",
    "        if self.datatype==\"test\":\n",
    "            pv = pd.DataFrame([pv.iloc[-1, :]])\n",
    "\n",
    "        return pv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worldcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worldcount(Features):\n",
    "    \"\"\"kernel features revised\n",
    "    \"\"\"\n",
    "    def __init__(self, train_labels, params, logger=None):        \n",
    "        super().__init__(params, logger=logger)\n",
    "        self.train_labels = train_labels        \n",
    "        \n",
    "    def calc_feature(self, org_train, org_test):                \n",
    "        if self.datatype == \"train\":\n",
    "            df = org_train\n",
    "            assess_user = df.loc[df.type == \"Assessment\"].installation_id.unique()\n",
    "            df = df.loc[df.installation_id.isin(assess_user)]\n",
    "        else:\n",
    "            # 直前までのnum_correct/incorrectを取得する\n",
    "            df = org_test\n",
    "                            \n",
    "        ret = applyParallel(df.groupby(\"installation_id\"), self.count_sessions)\n",
    "        ret_col = [c for c in list(ret.columns) if c not in [\"game_session\",\"installation_id\",\"title\",\"type\"]]\n",
    "        ret[ret_col] = ret[ret_col].fillna(0).astype(\"int32\")\n",
    "\n",
    "        use_cols = [c for c in list(ret.columns) if c not in [\"title\",\"type\", \"event_code\", \"gs_max_time\"]]\n",
    "        \n",
    "        self.format_and_save_feats(ret[use_cols])\n",
    "        \n",
    "        return ret[use_cols]\n",
    "    \n",
    "    def count_sessions(self, df):\n",
    "        world_cnt = self.sub_count_sessions(df, [\"world\"], \"wrd_cnt_\")\n",
    "        world_type_cnt = self.sub_count_sessions(df, [\"world\", \"type\"], \"wrd_type_cnt_\")        \n",
    "        title_type_cnt = self.sub_count_sessions(df, [\"title\", \"type\"], \"title_type_cnt_\")\n",
    "        \n",
    "        world_cnt = pd.merge(world_cnt, world_type_cnt, how=\"left\", on=[\"installation_id\", \"game_session\"])\n",
    "        del world_type_cnt\n",
    "        \n",
    "        world_cnt = pd.merge(world_cnt, title_type_cnt, how=\"left\", on=[\"installation_id\", \"game_session\"])\n",
    "        del title_type_cnt        \n",
    "        \n",
    "        return world_cnt\n",
    "    \n",
    "    def sub_count_sessions(self, df, group_columns, prefix):\n",
    "        \"\"\"session当該session直前までのactivityを示す\n",
    "        Args:\n",
    "            df: df grouped by installation_id \n",
    "        \"\"\"        \n",
    "        assess_sessions = df[df.type == \"Assessment\"][\"game_session\"].unique()\n",
    "        df[\"gs_max_time\"] = df.groupby(\"game_session\")[\"timestamp\"].transform(\"max\") # gs_max_timeでsortする必要がある\n",
    "\n",
    "        pv = pd.pivot_table(df, index=[\"installation_id\", \"gs_max_time\", \"game_session\"],  \n",
    "                            columns=group_columns, \n",
    "                            values=\"timestamp\", \n",
    "                            aggfunc=\"count\").fillna(0)\n",
    "\n",
    "        # 時刻順に並ぶことを保証する\n",
    "        pv.sort_values(\"gs_max_time\", ascending=True, inplace=True)\n",
    "        pv.reset_index(inplace=True)\n",
    "        \n",
    "        if len(group_columns) >= 2:\n",
    "            pv.columns = [c[0] + \"_\" + c[1] for c in pv.columns] \n",
    "            pv.rename(columns={\"installation_id_\": \"installation_id\", \n",
    "                               \"game_session_\": \"game_session\", \n",
    "                               \"gs_max_time_\": \"gs_max_time\"\n",
    "                              }, inplace=True)\n",
    "\n",
    "        cum_cols = [c for c in list(pv.columns) if c not in [\"installation_id\", \"game_session\", \"gs_max_time\"]]\n",
    "        pv[cum_cols] = pv[cum_cols].cumsum().shift(1).fillna(0).astype(\"int32\")\n",
    "\n",
    "\n",
    "        rename_dict = {}\n",
    "        for c in cum_cols:\n",
    "            rename_dict[c] = prefix + str(c)     \n",
    "        pv.rename(columns=rename_dict, inplace=True)\n",
    "        pv.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        del pv[\"gs_max_time\"]\n",
    "        gc.collect()\n",
    "        \n",
    "        if self.datatype==\"test\":\n",
    "            pv = pd.DataFrame([pv.iloc[-1, :]])\n",
    "        elif self.datatype ==\"train\":\n",
    "            pv = pv.loc[pv.game_session.isin(assess_sessions)]\n",
    "\n",
    "        return pv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SessionTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionTime(Features):\n",
    "    \"\"\"kernel features revised\n",
    "    \"\"\"\n",
    "    def __init__(self, train_labels, params, logger=None):        \n",
    "        super().__init__(params, logger=logger)\n",
    "        self.train_labels = train_labels        \n",
    "        \n",
    "    def calc_feature(self, org_train, org_test):                \n",
    "        if self.datatype == \"train\":\n",
    "            df = org_train\n",
    "            assess_user = df.loc[df.type == \"Assessment\"].installation_id.unique()\n",
    "            df = df.loc[df.installation_id.isin(assess_user)]\n",
    "        else:\n",
    "            # 直前までのnum_correct/incorrectを取得する\n",
    "            df = org_test\n",
    "                            \n",
    "        ret = applyParallel(df.groupby(\"installation_id\"), self.time_sessions)\n",
    "        use_cols = [c for c in list(ret.columns) if c not in [\"title\",\"type\", \"world\",\n",
    "                                                              \"event_code\", \"gs_max_time\", \"timestamp_max\", \"timestamp_min\"]]\n",
    "        self.format_and_save_feats(ret[use_cols])\n",
    "        \n",
    "        return ret[use_cols]\n",
    "    \n",
    "    def count_sessions(self, df):\n",
    "        world_cnt = self.sub_count_sessions(df, [\"world\"], \"wrd_cnt_\")\n",
    "        world_type_cnt = self.sub_count_sessions(df, [\"world\", \"type\"], \"wrd_type_cnt_\")        \n",
    "        title_type_cnt = self.sub_count_sessions(df, [\"title\", \"type\"], \"title_type_cnt_\")\n",
    "        \n",
    "        world_cnt = pd.merge(world_cnt, world_type_cnt, how=\"left\", on=[\"installation_id\", \"game_session\"])\n",
    "        del world_type_cnt\n",
    "        \n",
    "        world_cnt = pd.merge(world_cnt, title_type_cnt, how=\"left\", on=[\"installation_id\", \"game_session\"])\n",
    "        del title_type_cnt        \n",
    "        \n",
    "        return world_cnt\n",
    "    \n",
    "    def time_sessions(self, ins_df):\n",
    "        \"\"\"session当該session直前までのactivityを示す\n",
    "        Args:\n",
    "            df: df grouped by installation_id \n",
    "        \"\"\"        \n",
    "        agg_dict = {\n",
    "            \"timestamp\" : [\"max\", \"min\"]\n",
    "        }\n",
    "        duration_df = groupings(ins_df, [\"installation_id\", \"world\", \"type\", \"game_session\"], agg_dict).sort_values(\"timestamp_min\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "        duration_df[\"prev_gs_duration\"] = (duration_df[\"timestamp_max\"] - duration_df[\"timestamp_min\"]).shift(1).dt.total_seconds()\n",
    "        duration_df[\"session_interval\"] = (duration_df[\"timestamp_min\"] - duration_df[\"timestamp_max\"].shift(1)).dt.total_seconds()\n",
    "        \n",
    "        window = 25\n",
    "        min_periods = 5\n",
    "        for col in [\"prev_gs_duration\", \"session_interval\"]:\n",
    "            duration_df[col + \"rmean\"] = duration_df[col].rolling(window=window, min_periods=min_periods).mean()\n",
    "            duration_df[col + \"rstd\"] = duration_df[col].rolling(window=window, min_periods=min_periods).std()\n",
    "            duration_df[col + \"rmax\"] = duration_df[col].rolling(window=window, min_periods=2).max()\n",
    "            duration_df[col + \"rmin\"] = duration_df[col].rolling(window=window, min_periods=2).min()        \n",
    "            \n",
    "        duration_df = duration_df.loc[duration_df.type == \"Assessment\"]\n",
    "        \n",
    "        if self.datatype==\"test\":\n",
    "            duration_df = pd.DataFrame([duration_df.iloc[-1, :]])\n",
    "\n",
    "        return duration_df\n",
    "    \n",
    "    \n",
    "class SessionTime2(Features):\n",
    "    \"\"\"kernel features revised\n",
    "    \"\"\"\n",
    "    def __init__(self, train_labels, params, logger=None):        \n",
    "        super().__init__(params, logger=logger)\n",
    "        self.train_labels = train_labels        \n",
    "        \n",
    "    def calc_feature(self, org_train, org_test):                \n",
    "        if self.datatype == \"train\":\n",
    "            df = org_train\n",
    "            df = df.loc[df.installation_id.isin(self.train_labels.installation_id.unique())]\n",
    "        else:\n",
    "            # 直前までのnum_correct/incorrectを取得する\n",
    "            df = org_test\n",
    "                            \n",
    "        ret = applyParallel(df.groupby(\"installation_id\"), self.time_sessions)\n",
    "        use_cols = [c for c in list(ret.columns) if c not in [\"title\",\"type\", \"world\",\n",
    "                                                              \"event_code\", \"gs_max_time\", \"timestamp_max\", \"timestamp_min\"]]\n",
    "        self.format_and_save_feats(ret[use_cols])\n",
    "        \n",
    "        return ret[use_cols]\n",
    "    \n",
    "    def time_sessions(self, ins_df):\n",
    "        \"\"\"session当該session直前までのactivityを示す\n",
    "        Args:\n",
    "            df: df grouped by installation_id \n",
    "        \"\"\"        \n",
    "        ### session feature for all \"type\"\n",
    "        agg_dict = {\n",
    "            \"timestamp\" : [\"max\", \"min\"]\n",
    "        }\n",
    "        duration_df = groupings(ins_df, [\"installation_id\", \"world\", \"type\", \"game_session\"], agg_dict).sort_values(\"timestamp_min\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "        duration_df[\"prev_gs_duration\"] = (duration_df[\"timestamp_max\"] - duration_df[\"timestamp_min\"]).shift(1).dt.total_seconds()\n",
    "        duration_df[\"session_interval\"] = (duration_df[\"timestamp_min\"] - duration_df[\"timestamp_max\"].shift(1)).dt.total_seconds()\n",
    "        \n",
    "        window = 5\n",
    "        min_periods = 2\n",
    "        for col in [\"prev_gs_duration\", \"session_interval\"]:\n",
    "            duration_df[col + \"rmean\"] = duration_df[col].rolling(window=window, min_periods=min_periods).mean()\n",
    "            duration_df[col + \"rstd\"] = duration_df[col].rolling(window=window, min_periods=min_periods).std()\n",
    "            duration_df[col + \"rmax\"] = duration_df[col].rolling(window=window, min_periods=2).max()\n",
    "            duration_df[col + \"rmin\"] = duration_df[col].rolling(window=window, min_periods=2).min()        \n",
    "            \n",
    "        duration_df = duration_df.loc[duration_df.type == \"Assessment\"]\n",
    "        \n",
    "        ### session feature for \"assessments\"        \n",
    "        agg_dict = {\n",
    "            \"timestamp\" : [\"max\", \"min\"]\n",
    "        }\n",
    "        ass_duration = groupings(ins_df, [\"installation_id\", \"world\", \"type\", \"game_session\"], agg_dict)\n",
    "        ass_duration = ass_duration.loc[ass_duration.type == \"Assessment\"].sort_values(\"timestamp_min\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "        ass_duration[\"prev_ass_gs_duration\"] = (ass_duration[\"timestamp_max\"] - ass_duration[\"timestamp_min\"]).shift(1).dt.total_seconds()\n",
    "        ass_duration[\"ass_session_interval\"] = (ass_duration[\"timestamp_min\"] - ass_duration[\"timestamp_max\"].shift(1)).dt.total_seconds()\n",
    "\n",
    "        window = 5\n",
    "        min_periods = 1\n",
    "        for col in [\"prev_ass_gs_duration\", \"ass_session_interval\"]:\n",
    "            ass_duration[col + \"_rmean\"] = ass_duration[col].rolling(window=window, min_periods=min_periods).mean()\n",
    "            ass_duration[col + \"_rstd\"] = ass_duration[col].rolling(window=window, min_periods=min_periods).std()\n",
    "            ass_duration[col + \"_rmax\"] = ass_duration[col].rolling(window=window, min_periods=1).max()\n",
    "            ass_duration[col + \"_rmin\"] = ass_duration[col].rolling(window=window, min_periods=1).min()        \n",
    "        \n",
    "        ass_cols = [c for c in list(ass_duration.columns) if c not in ['installation_id', 'world', 'type','timestamp_max','timestamp_min']]\n",
    "        \n",
    "        duration_df = pd.merge(duration_df, ass_duration[ass_cols], how=\"left\", on=\"game_session\")\n",
    "        \n",
    "        if self.datatype==\"test\":\n",
    "            duration_df = pd.DataFrame([duration_df.iloc[-1, :]])\n",
    "\n",
    "        return duration_df    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EncodingTitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodingTitles(Features):\n",
    "    \"\"\"Event count in only Assessments\n",
    "    \"\"\"\n",
    "    def __init__(self, train_labels, params, logger=None):        \n",
    "        super().__init__(params, logger=logger)\n",
    "        self.train_labels = train_labels        \n",
    "        \n",
    "    def get_encoder(self, org_train, org_test):\n",
    "        self.all_activities = set(org_train[\"title\"].unique()).union(\n",
    "            set(org_test[\"title\"].unique()))\n",
    "        self.all_event_codes = set(org_train[\"event_code\"].unique()).union(\n",
    "            org_test[\"event_code\"].unique())\n",
    "        self.activities_map = dict(\n",
    "            zip(self.all_activities, np.arange(len(self.all_activities))))\n",
    "        self.inverse_activities_map = dict(\n",
    "            zip(np.arange(len(self.all_activities)), self.all_activities))\n",
    "        \n",
    "    def calc_feature(self, org_train, org_test):                \n",
    "        if self.datatype == \"train\":\n",
    "            df = org_train\n",
    "            assess_user = df.loc[df.type == \"Assessment\"].installation_id.unique()\n",
    "            df = df.loc[df.installation_id.isin(assess_user)]\n",
    "        else:\n",
    "            # 直前までのnum_correct/incorrectを取得する\n",
    "            org_test.loc[(org_test.event_code.isin([4100, 4110])) & (org_test[\"event_data\"].str.contains(\"true\")), 'num_correct'] = 1\n",
    "            org_test.loc[(org_test.event_code.isin([4100, 4110])) & (org_test[\"event_data\"].str.contains(\"false\")), 'num_incorrect'] = 1    \n",
    "            df = org_test\n",
    "        \n",
    "        # get encodings informations\n",
    "        self.get_encoder(org_train, org_test)\n",
    "        \n",
    "        ret = applyParallel(df.groupby(\"installation_id\"), self.ins_id_sessions)\n",
    "        use_cols = [c for c in list(ret.columns) if c not in [\"accuracy\",\"accuracy_group\",\"cum_accuracy\",\"title\",\n",
    "                                                             \"type\", \"event_code\", \"gs_max_time\"\n",
    "                                                            ]]\n",
    "        \n",
    "        self.format_and_save_feats(ret[use_cols])\n",
    "        return ret[use_cols]\n",
    "    \n",
    "    def ins_id_sessions(self, df):\n",
    "        \"\"\"session当該session直前までのactivityを示す\n",
    "        Args:\n",
    "            df: df grouped by installation_id \n",
    "        \"\"\"\n",
    "        df[\"title_enc\"] = df[\"title\"].map(self.activities_map)\n",
    "        df = df.loc[df.type==\"Assessment\"][[\"installation_id\", \"game_session\", \"title_enc\"]].drop_duplicates().reset_index(drop=True)\n",
    "        \n",
    "        if self.datatype==\"test\":\n",
    "            df = pd.DataFrame([df.iloc[-1, :]])\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PrevAssessResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrevAssessResult(Features):\n",
    "    \"\"\"kernel features revised\n",
    "    \"\"\"\n",
    "    def __init__(self, train_labels, params, logger=None):        \n",
    "        super().__init__(params, logger=logger)\n",
    "        self.train_labels = train_labels        \n",
    "        \n",
    "    def calc_feature(self, org_train, org_test):                \n",
    "        if self.datatype == \"train\":\n",
    "            df = org_train\n",
    "            assess_user = df.loc[df.type == \"Assessment\"].installation_id.unique()\n",
    "            df = df.loc[df.installation_id.isin(assess_user)]\n",
    "        else:\n",
    "            # 直前までのnum_correct/incorrectを取得する\n",
    "            org_test.loc[(org_test.event_code.isin([4100, 4110])) & (org_test[\"event_data\"].str.contains(\"true\")), 'num_correct'] = 1\n",
    "            org_test.loc[(org_test.event_code.isin([4100, 4110])) & (org_test[\"event_data\"].str.contains(\"false\")), 'num_incorrect'] = 1    \n",
    "            df = org_test\n",
    "        \n",
    "        ret = applyParallel(df.groupby(\"installation_id\"), self.ins_id_sessions)\n",
    "        ret_col = [c for c in list(ret.columns) if c not in [\"accuracy\",\"accuracy_group\",\"cum_accuracy\",\n",
    "                                                             \"game_session\",\"installation_id\",\"title\",\n",
    "                                                             \"type\"\n",
    "                                                            ]]\n",
    "        ret[ret_col] = ret[ret_col].fillna(0).astype(\"int32\")\n",
    "#         self.format_and_save_feats(ret)\n",
    "\n",
    "        use_cols = [c for c in list(ret.columns) if c not in [\"accuracy\",\"accuracy_group\",\"cum_accuracy\",\"title\",\n",
    "                                                             \"type\", \"event_code\", \"gs_max_time\"\n",
    "                                                            ]]\n",
    "        \n",
    "        self.format_and_save_feats(ret[use_cols])\n",
    "        \n",
    "        return ret[use_cols]\n",
    "    \n",
    "    def ins_id_sessions(self, df):\n",
    "        \"\"\"session当該session直前までのactivityを示す\n",
    "        Args:\n",
    "            df: df grouped by installation_id \n",
    "        \"\"\"\n",
    "        # 単純なactivity count\n",
    "        df[\"gs_max_time\"] = df.groupby(\"game_session\")[\"timestamp\"].transform(\"max\") # gs_max_timeでsortする必要がある\n",
    "        pv = pd.pivot_table(df, index=[\"installation_id\", \"gs_max_time\", \"game_session\", \"type\"],  \n",
    "                            columns=\"title\", \n",
    "                            values=\"timestamp\", \n",
    "                            aggfunc=\"count\").fillna(0)\n",
    "\n",
    "\n",
    "        assess_col = [c for c in list(pv.columns) if \"Assessment\" in c]\n",
    "        pv = pv[assess_col]\n",
    "        pv.reset_index(inplace=True)\n",
    "\n",
    "        rename_dict = {}\n",
    "        new_cols = []\n",
    "        \n",
    "        cnt_pref = \"assess_cnt_\"\n",
    "        for c in assess_col:\n",
    "            rename_dict[c] = cnt_pref + str(c)     \n",
    "\n",
    "        pv = pv.loc[pv.type==\"Assessment\"].reset_index(drop=True)\n",
    "        pv.sort_values(\"gs_max_time\", ascending=True, inplace=True)\n",
    "        pv.reset_index(inplace=True, drop=True)\n",
    "        pv[assess_col] = pv[assess_col].shift(1).fillna(0)\n",
    "        pv.rename(columns=rename_dict, inplace=True)\n",
    "        \n",
    "        for c in assess_col:\n",
    "            pv[\"accum\" + cnt_pref + str(c)] = pv[cnt_pref + str(c)].cumsum()\n",
    "\n",
    "        del pv[\"gs_max_time\"], pv[\"type\"]\n",
    "\n",
    "        return pv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PrevAssessAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_history(gr_df):\n",
    "    gr_df = gr_df.sort_values(\"gs_max_time\", ascending=True)\n",
    "\n",
    "    gr_df[\"as_acc_c_num\"] = gr_df[\"num_correct\"].cumsum()\n",
    "    gr_df[\"as_acc_inc_num\"] = gr_df[\"num_incorrect\"].cumsum()\n",
    "    gr_df[\"as_prev_acc\"] = gr_df[\"num_correct\"] / (gr_df[\"num_correct\"] + gr_df[\"num_incorrect\"])\n",
    "    gr_df[\"as_cum_acc\"] = gr_df[\"as_acc_c_num\"] / (gr_df[\"as_acc_c_num\"] + gr_df[\"as_acc_inc_num\"])\n",
    "\n",
    "    shift_col = [\"num_correct\", \"num_incorrect\", \"as_acc_c_num\", \"as_acc_inc_num\", \"as_prev_acc\", \"as_cum_acc\"]\n",
    "    gr_df[shift_col] = gr_df[shift_col].shift(1).fillna(-99)\n",
    "\n",
    "    return gr_df    \n",
    "\n",
    "class PrevAssessAcc(Features):\n",
    "    \"\"\"kernel features revised\n",
    "    \"\"\"\n",
    "    def __init__(self, train_labels, params, logger=None):        \n",
    "        super().__init__(params, logger=logger)\n",
    "        self.train_labels = train_labels        \n",
    "        \n",
    "    def calc_feature(self, org_train, org_test):                \n",
    "        if self.datatype == \"train\":\n",
    "            df = org_train\n",
    "            assess_user = df.loc[df.type == \"Assessment\"].installation_id.unique()\n",
    "            df = df.loc[df.installation_id.isin(assess_user)]\n",
    "        else:\n",
    "            # 直前までのnum_correct/incorrectを取得する\n",
    "            df = org_test\n",
    "        \n",
    "        ret = applyParallel(df.groupby(\"installation_id\"), self.ins_id_sessions)\n",
    "        ret_col = [c for c in list(ret.columns) if c not in [\"accuracy\",\"accuracy_group\",\"cum_accuracy\",\n",
    "                                                             \"game_session\",\"installation_id\",\"title\",\n",
    "                                                             \"type\"\n",
    "                                                            ]]\n",
    "        ret[ret_col] = ret[ret_col].fillna(0)\n",
    "#         self.format_and_save_feats(ret)\n",
    "\n",
    "        use_cols = [c for c in list(ret.columns) if c not in [\"accuracy\",\"accuracy_group\",\"cum_accuracy\",\"title\",\n",
    "                                                             \"type\", \"event_code\", \"gs_max_time\"\n",
    "                                                            ]]\n",
    "        self.format_and_save_feats(ret[use_cols])\n",
    "        \n",
    "        return ret[use_cols]\n",
    "        \n",
    "    def ins_id_sessions(self, df):\n",
    "        \"\"\"session当該session直前までのactivityを示す\n",
    "        Args:\n",
    "            df: df grouped by installation_id \n",
    "        \"\"\"\n",
    "        # 単純なactivity count\n",
    "        df[\"gs_max_time\"] = df.groupby(\"game_session\")[\"timestamp\"].transform(\"max\") # gs_max_timeでsortする必要がある\n",
    "        \n",
    "        c_ass_idx = ((df.event_code == 4100) \n",
    "                          & (df.title != \"Bird Measurer (Assessment)\") \n",
    "                          & (df[\"event_data\"].str.contains(\"true\"))) | \\\n",
    "                         ((df.event_code == 4110) \n",
    "                          & (df.title == \"Bird Measurer (Assessment)\") \n",
    "                          & (df[\"event_data\"].str.contains(\"true\")))\n",
    "\n",
    "        inc_ass_idx = ((df.event_code == 4100) \n",
    "                          & (df.title != \"Bird Measurer (Assessment)\") \n",
    "                          & (df[\"event_data\"].str.contains(\"false\"))) | \\\n",
    "                         ((df.event_code == 4110) \n",
    "                          & (df.title == \"Bird Measurer (Assessment)\") \n",
    "                          & (df[\"event_data\"].str.contains(\"false\")))\n",
    "\n",
    "        df.loc[c_ass_idx, 'num_correct'] = 1\n",
    "        df.loc[inc_ass_idx, 'num_incorrect'] = 1\n",
    "\n",
    "        df[\"num_correct\"].fillna(0, inplace=True)\n",
    "        df[\"num_incorrect\"].fillna(0, inplace=True)\n",
    "\n",
    "        df = df.loc[(df.type ==\"Assessment\")]\n",
    "        \n",
    "        df = df.groupby([\"installation_id\", \"game_session\", \"gs_max_time\", \"title\"])[[\"num_correct\", \"num_incorrect\"]].sum().reset_index()\n",
    "               \n",
    "        df = df.groupby(\"title\").apply(assess_history)\n",
    "        df = df.sort_values(\"gs_max_time\", ascending=True)\n",
    "\n",
    "        del df[\"title\"], df[\"gs_max_time\"]\n",
    "        \n",
    "        if self.datatype==\"test\":\n",
    "            df = pd.DataFrame([df.iloc[-1, :]])\n",
    "       \n",
    "        return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PrevAssessAccByTitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrevAssessAccByTitle(Features):\n",
    "    \"\"\"kernel features revised\n",
    "    \"\"\"\n",
    "    def __init__(self, train_labels, params, logger=None):        \n",
    "        super().__init__(params, logger=logger)\n",
    "        self.train_labels = train_labels        \n",
    "        \n",
    "    def calc_feature(self, org_train, org_test):                \n",
    "        if self.datatype == \"train\":\n",
    "            df = org_train\n",
    "            df = df.loc[df.installation_id.isin(self.train_labels.installation_id.unique())]\n",
    "        else:\n",
    "            # 直前までのnum_correct/incorrectを取得する\n",
    "            df = org_test\n",
    "        \n",
    "        ret = applyParallel(df.groupby(\"installation_id\"), self.ins_id_sessions)\n",
    "        ret_col = [c for c in list(ret.columns) if c not in [\"accuracy\",\"accuracy_group\",\"cum_accuracy\",\n",
    "                                                             \"game_session\",\"installation_id\",\"title\",\n",
    "                                                             \"type\"\n",
    "                                                            ]]\n",
    "        ret[ret_col] = ret[ret_col].fillna(0)\n",
    "#         self.format_and_save_feats(ret)\n",
    "\n",
    "        use_cols = [c for c in list(ret.columns) if c not in [\"accuracy\",\"accuracy_group\",\"cum_accuracy\",\"title\",\n",
    "                                                             \"type\", \"event_code\", \"gs_max_time\"\n",
    "                                                            ]]\n",
    "        self.format_and_save_feats(ret[use_cols])\n",
    "        \n",
    "        return ret[use_cols]\n",
    "        \n",
    "    def ins_id_sessions(self, df):\n",
    "        \"\"\"session当該session直前までのactivityを示す\n",
    "        Args:\n",
    "            df: df grouped by installation_id \n",
    "        \"\"\"\n",
    "        # 単純なactivity count\n",
    "        df[\"gs_max_time\"] = df.groupby(\"game_session\")[\"timestamp\"].transform(\"max\") # gs_max_timeでsortする必要がある\n",
    "        \n",
    "        c_ass_idx = ((df.event_code == 4100) \n",
    "                          & (df.title != \"Bird Measurer (Assessment)\") \n",
    "                          & (df[\"event_data\"].str.contains(\"true\"))) | \\\n",
    "                         ((df.event_code == 4110) \n",
    "                          & (df.title == \"Bird Measurer (Assessment)\") \n",
    "                          & (df[\"event_data\"].str.contains(\"true\")))\n",
    "\n",
    "        inc_ass_idx = ((df.event_code == 4100) \n",
    "                          & (df.title != \"Bird Measurer (Assessment)\") \n",
    "                          & (df[\"event_data\"].str.contains(\"false\"))) | \\\n",
    "                         ((df.event_code == 4110) \n",
    "                          & (df.title == \"Bird Measurer (Assessment)\") \n",
    "                          & (df[\"event_data\"].str.contains(\"false\")))\n",
    "\n",
    "        df.loc[c_ass_idx, 'num_correct'] = 1\n",
    "        df.loc[inc_ass_idx, 'num_incorrect'] = 1\n",
    "\n",
    "        df[\"num_correct\"].fillna(0, inplace=True)\n",
    "        df[\"num_incorrect\"].fillna(0, inplace=True)\n",
    "\n",
    "        df = df.loc[(df.type ==\"Assessment\")]\n",
    "        \n",
    "        pv = pd.pivot_table(df, index=[\"installation_id\", \"game_session\", \"gs_max_time\"], columns=\"title\", values=[\"num_correct\", \"num_incorrect\"], aggfunc=\"sum\").reset_index().sort_values(\"gs_max_time\")\n",
    "\n",
    "        pv.columns = [c[0] + \"_\" + c[1] if c[1] != \"\" else c[0] for c in list(pv.columns)]\n",
    "        pv_num_cols = [c for c in list(pv.columns) if \"correct\" in c]\n",
    "\n",
    "        cum_cols = [\"cum_\" + c for c in pv_num_cols] # 累積列\n",
    "\n",
    "        pv_cum_corr_cols = [c for c in cum_cols if \"cum_num_correct_\" in c] # correct 列のみ \n",
    "        pv_cum_incorr_cols = [c for c in cum_cols if \"cum_num_incorrect_\" in c] # incorrect 列\n",
    "        pv_cum_acc_cols = [\"cum_acc_\" + re.sub('num_incorrect_', '', c) for c in pv_cum_incorr_cols]\n",
    "\n",
    "        pv.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        pv[pv_num_cols] = pv[pv_num_cols].shift(1).fillna(0)\n",
    "        pv[cum_cols] = pv[pv_num_cols].cumsum()\n",
    "\n",
    "        pv[pv_cum_acc_cols] = pd.DataFrame(pv[pv_cum_corr_cols].values / (pv[pv_cum_corr_cols].values + pv[pv_cum_incorr_cols].values))\n",
    "\n",
    "        del pv[\"gs_max_time\"]\n",
    "        \n",
    "        if self.datatype==\"test\":\n",
    "            pv = pd.DataFrame([pv.iloc[-1, :]])\n",
    "       \n",
    "        return pv\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GameDurMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_duration(val):\n",
    "    val = json.loads(val)\n",
    "    duration = val[\"duration\"]\n",
    "    g_misses = val[\"misses\"]\n",
    "    \n",
    "    return [duration, g_misses]\n",
    "\n",
    "def world_cum_duration_calc(world_df):\n",
    "    # duration / missを抽出\n",
    "    wg_df = world_df[(world_df.event_code == 2030) & (world_df.type==\"Game\")]\n",
    "    du_miss = np.array(wg_df[\"event_data\"].apply(game_duration).tolist())\n",
    "    try:\n",
    "        wg_df[\"duration\"] = du_miss[:, 0]\n",
    "        wg_df[\"misses\"] = du_miss[:, 1]\n",
    "    except:\n",
    "        wg_df[\"duration\"] = np.nan\n",
    "        wg_df[\"misses\"] = np.nan\n",
    "\n",
    "    del du_miss\n",
    "\n",
    "    aggs = {\n",
    "        \"duration\": [\"min\", \"mean\", \"max\", \"std\", \"count\"],\n",
    "        \"misses\": [\"min\", \"mean\", \"max\", \"std\"],\n",
    "    }\n",
    "\n",
    "    game_cums = groupings(wg_df, [\"game_session\", \"gs_max_time\", \"world\"], aggs, \"g_\")\n",
    "\n",
    "    del wg_df\n",
    "    gc.collect()\n",
    "\n",
    "    # 累積を計算\n",
    "    game_cums = game_cums.sort_values(\"gs_max_time\").reset_index(drop=True)\n",
    "\n",
    "    num_cols = [c for c in list(game_cums.columns) if c not in [\"game_session\", \"gs_max_time\", \"world\"] ]\n",
    "    cum_mean_cols = [\"mean_\" + c for c in num_cols]\n",
    "\n",
    "    game_cums[cum_mean_cols] = game_cums[num_cols].cumsum()\n",
    "    game_cums[\"cumnum\"] = (game_cums.index + 1).values\n",
    "    game_cums[cum_mean_cols] /= game_cums[\"cumnum\"].values.reshape((-1, 1))\n",
    "\n",
    "    game_cums[[\"game_session\", \"gs_max_time\", \"world\"] + cum_mean_cols]\n",
    "\n",
    "    # 直前のgameまでの累積結果をmergeする\n",
    "    game_ass_uni = world_df[[\"world\", \"game_session\", \"type\", \"installation_id\",\"gs_max_time\"]].drop_duplicates().sort_values(\"gs_max_time\").reset_index(drop=True)\n",
    "\n",
    "    game_ass_uni = pd.merge(game_ass_uni, game_cums, how=\"left\", on=[\"game_session\", \"gs_max_time\", \"world\"]).fillna(method=\"ffill\")\n",
    "    game_ass_uni = game_ass_uni.loc[game_ass_uni.type==\"Assessment\"]\n",
    "\n",
    "    return game_ass_uni\n",
    "\n",
    "class GameDurMiss(Features):\n",
    "    \"\"\"assessment 直前までのgameのプレイ状況を取得する\n",
    "    \"\"\"\n",
    "    def __init__(self, train_labels, params, logger=None):        \n",
    "        super().__init__(params, logger=logger)\n",
    "        self.train_labels = train_labels        \n",
    "        \n",
    "    def calc_feature(self, org_train, org_test):                \n",
    "        if self.datatype == \"train\":\n",
    "            df = org_train\n",
    "            df = df.loc[df.installation_id.isin(self.train_labels.installation_id.unique())]\n",
    "        else:\n",
    "            # 直前までのnum_correct/incorrectを取得する\n",
    "            df = org_test\n",
    "        \n",
    "        ret = applyParallel(df.groupby(\"installation_id\"), self.ins_id_sessions)\n",
    "        ret_col = [c for c in list(ret.columns) if c not in [\"accuracy\",\"accuracy_group\",\"cum_accuracy\",\n",
    "                                                             \"game_session\",\"installation_id\",\"title\",\n",
    "                                                             \"type\"\n",
    "                                                            ]]\n",
    "        \n",
    "        use_cols = [c for c in list(ret.columns) if c not in [\"accuracy\",\"accuracy_group\",\"cum_accuracy\",\"title\",\n",
    "                                                             \"type\", \"event_code\", \"gs_max_time\"\n",
    "                                                            ]]\n",
    "        self.format_and_save_feats(ret[use_cols])\n",
    "        \n",
    "        return ret[use_cols]\n",
    "        \n",
    "    def ins_id_sessions(self, df):\n",
    "        \"\"\"assessment 直前までのgameのプレイ状況を取得する\n",
    "        Args:\n",
    "            df: df grouped by installation_id \n",
    "        \"\"\"\n",
    "        # 単純なactivity count\n",
    "        gs_game_ass = df.loc[((df.event_code == 2030) & (df.type==\"Game\")) | (df.type==\"Assessment\")]\n",
    "        gs_game_ass[\"gs_max_time\"] = gs_game_ass.groupby(\"game_session\")[\"timestamp\"].transform(\"max\")        \n",
    "        \n",
    "        game_ass_uni = gs_game_ass.groupby(\"world\").apply(world_cum_duration_calc).reset_index(drop=True).sort_values(\"gs_max_time\")\n",
    "        \n",
    "        del gs_game_ass\n",
    "        \n",
    "        if self.datatype==\"test\":\n",
    "            game_ass_uni = pd.DataFrame([game_ass_uni.iloc[-1, :]])\n",
    "       \n",
    "        return game_ass_uni\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Rounder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import scipy as sp\n",
    "\n",
    "class OptimizedRounder(object):\n",
    "    \"\"\"\n",
    "    An optimizer for rounding thresholds\n",
    "    to maximize Quadratic Weighted Kappa (QWK) score\n",
    "    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        \"\"\"\n",
    "        Get loss according to\n",
    "        using current coefficients\n",
    "        \n",
    "        :param coef: A list of coefficients that will be used for rounding\n",
    "        :param X: The raw predictions\n",
    "        :param y: The ground truth labels\n",
    "        \"\"\"\n",
    "        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "\n",
    "        return -qwk(y, X_p)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Optimize rounding thresholds\n",
    "        \n",
    "        :param X: The raw predictions\n",
    "        :param y: The ground truth labels\n",
    "        \"\"\"\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        \"\"\"\n",
    "        Make predictions with specified thresholds\n",
    "        \n",
    "        :param X: The raw predictions\n",
    "        :param coef: A list of coefficients that will be used for rounding\n",
    "        \"\"\"\n",
    "        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "\n",
    "    def coefficients(self):\n",
    "        \"\"\"\n",
    "        Return the optimized coefficients\n",
    "        \"\"\"\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess \n",
    "\n",
    "## add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dfs(use_features, is_local=False, logger=None, debug=True):\n",
    "    # read dataframes     \n",
    "    with timer(\"read datasets\"):\n",
    "        if debug: \n",
    "            nrows=200000 \n",
    "        else: nrows=None\n",
    "            \n",
    "        sub = pd.read_csv(base_path + '/sample_submission.csv')\n",
    "        \n",
    "        if is_local:\n",
    "            org_train = pickle_load(\"../input/train.pkl\")\n",
    "            org_test = pickle_load(\"../input/test.pkl\")\n",
    "        else:\n",
    "            org_train = pd.read_csv(base_path + \"/train.csv\", nrows=nrows)\n",
    "            org_test = pd.read_csv(base_path + \"/test.csv\", nrows=nrows)\n",
    "            \n",
    "        org_train = memory_reducer(org_train, verbose=True)\n",
    "        org_test = org_test[org_test.installation_id.isin(sub.installation_id)]\n",
    "        org_test.sort_values(['installation_id', 'timestamp'], inplace=True)\n",
    "        org_test.reset_index(inplace=True)\n",
    "        org_test = memory_reducer(org_test, verbose=True)\n",
    "        \n",
    "        train_labels = pd.read_csv(base_path + \"/train_labels.csv\", nrows=nrows)\n",
    "        specs = pd.read_csv(base_path + \"/specs.csv\", nrows=nrows)\n",
    "\n",
    "    # basic preprocess\n",
    "    org_train[\"timestamp\"] = pd.to_datetime(org_train[\"timestamp\"])\n",
    "    org_test[\"timestamp\"] = pd.to_datetime(org_test[\"timestamp\"])\n",
    "    \n",
    "    with timer(\"merging features\"):\n",
    "        train_df = add_features(use_features, org_train, org_test, train_labels, specs, datatype=\"train\", is_local=is_local, logger=None)\n",
    "        train_df = train_df.reset_index(drop=True)\n",
    "        test_df = add_features(use_features, org_train, org_test, train_labels, specs, datatype=\"test\", is_local=is_local, logger=None)\n",
    "        test_df = test_df.reset_index(drop=True)\n",
    "    \n",
    "#     df = pd.concat([df, feat_df], axis=1)\n",
    "    print(\"preprocess done!!\")\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def feature_maker(feat_cls, is_overwrite, org_train, org_test, train_labels, params, logger, is_local):\n",
    "    \"\"\"featureの読み込み\n",
    "    \"\"\"\n",
    "    feat_ = feat_cls(train_labels, params, logger)\n",
    "    feat_name = feat_.name\n",
    "    datatype = feat_.datatype\n",
    "    feature_dir = os.path.join(os.path.dirname(\"__file__\"), \"../feature\")\n",
    "    feature_path = Path(feature_dir) / f\"{datatype}\" / f\"{feat_name}.pkl\"\n",
    "    \n",
    "    if os.path.exists(feature_path) and is_overwrite is False:\n",
    "        f_df = pickle_load(feature_path)\n",
    "    else:\n",
    "        f_df = feat_.feature_extract(org_train, org_test)\n",
    "    \n",
    "    return f_df\n",
    "\n",
    "\n",
    "def add_features(use_features, org_train, org_test, train_labels, specs, datatype, is_local=False, logger=None):\n",
    "    # 都度計算する\n",
    "    feat_params = {\n",
    "        \"datatype\": datatype,\n",
    "        \"debug\": True,\n",
    "        \"is_overwrite\": True,\n",
    "    }\n",
    "\n",
    "    # base feature\n",
    "    base_feat = KernelBasics2(train_labels, feat_params, logger)\n",
    "    feature_dir = os.path.join(os.path.dirname(\"__file__\"), \"../feature\")\n",
    "    feature_path = Path(feature_dir) / f\"{datatype}\" / f\"{base_feat.name}.pkl\"\n",
    "    \n",
    "    if os.path.exists(feature_path):\n",
    "        feat_df = pickle_load(feature_path)\n",
    "    else:\n",
    "        feat_df = base_feat.feature_extract(org_train, org_test)\n",
    "\n",
    "    # add event_counts\n",
    "    for name, feat_condition in use_features.items():\n",
    "        feat_cls = feat_condition[0]\n",
    "        is_overwrite = feat_condition[1]\n",
    "        \n",
    "        f_df = feature_maker(feat_cls, is_overwrite, org_train, org_test, train_labels, feat_params, logger, is_local)\n",
    "        feat_df = pd.merge(feat_df, f_df, how=\"left\", on =[\"installation_id\", \"game_session\"])\n",
    "        del f_df\n",
    "\n",
    "    return feat_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_feature = {\n",
    "    \"EventCount\": [EventCount, True], # class, is_overwrite\n",
    "    \"EventCount2\": [EventCount2, True], # class, is_overwrite\n",
    "    \"Worldcount\": [Worldcount, True],\n",
    "    \"SessionTime\": [SessionTime2, True],\n",
    "#     \"AssessEventCount\": [AssessEventCount, False],\n",
    "    \"EncodingTitles\": [EncodingTitles, True],\n",
    "#     \"PrevAssessResult\":[PrevAssessResult, True],\n",
    "#     \"PrevAssessAcc\": [PrevAssessAcc, True],\n",
    "    \"PrevAssessAccByTitle\": [PrevAssessAccByTitle, True]\n",
    "}\n",
    "\n",
    "is_local = False\n",
    "\n",
    "if is_local:\n",
    "    base_path = \"../input\" # at local\n",
    "    train_df, test_df = preprocess_dfs(use_feature, is_local=is_local, logger=None, debug=False)\n",
    "    \n",
    "else:\n",
    "    sub = pd.read_csv('../input/data-science-bowl-2019/sample_submission.csv')\n",
    "    base_path = '/kaggle/input/data-science-bowl-2019' # at kaggle kernel\n",
    "    if len(sub)==1000:\n",
    "        sub.to_csv('submission.csv', index=False)\n",
    "        exit(0)\n",
    "    else:\n",
    "        train_df, test_df = preprocess_dfs(use_feature, is_local=is_local, logger=None, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"suga_001_add_eventidcnt\"\n",
    "logger, log_path = log_output(exp_name)\n",
    "\n",
    "train_small_dataset = False\n",
    "is_debug = True\n",
    "\n",
    "# train_params = {\n",
    "#     'learning_rate': 0.01,\n",
    "#     'bagging_fraction': 0.90,\n",
    "#     'feature_fraction': 0.85,\n",
    "#     'max_depth': 5,\n",
    "#     'lambda_l1': 0.7,\n",
    "#     'lambda_l2': 0.7,\n",
    "#     'metric': 'multiclass',\n",
    "#     'objective': 'multiclass',\n",
    "#     'num_classes': 4,\n",
    "#     'random_state': 773,\n",
    "#     \"n_estimators\": 3000    \n",
    "\n",
    "# }\n",
    "\n",
    "train_params = {\n",
    "     'learning_rate': 0.01,\n",
    "     'boosting_type': 'gbdt',\n",
    "     'objective': 'regression',\n",
    "     'metric': 'rmse',\n",
    "    'num_leaves':  64,\n",
    "     'bagging_fraction': 0.9,\n",
    "     'bagging_freq': 1,\n",
    "     'feature_fraction': 0.7,\n",
    "     'max_depth': -1,\n",
    "     'lambda_l1': 0.2,\n",
    "     'lambda_l2': 0.4,\n",
    "     'seed': 19930802,\n",
    "     'n_estimators': 100000\n",
    "}\n",
    "\n",
    "bad_feats = [\n",
    "    'prev_gs_duration', 'session_intervalrmin', 'session_intervalrstd', 'session_intervalrmax', 'session_interval', 'accum_acc_gr_-99',\n",
    "    'session_intervalrmean', 'ass_session_interval', 'prev_gs_durationrmean', 'prev_gs_durationrmax',\n",
    "    'ev_cnt4070', 'prev_gs_durationrstd', 'mean_g_duration_meaan', 'ev_cnt3010', 'g_duration_std', 'ev_cnt4030', 'ev_cnt3110',\n",
    "    'g_duration_mean', 'meaan_g_duration_min', 'ass_session_interval_rmin', 'accum_acc_gr_3', 'g_duration_min', 'mean_g_duraation_std'\n",
    "]\n",
    "no_use_cols = [\n",
    "    \"accuracy\",\n",
    "    \"accuracy_group\",\n",
    "    \"game_session\",\n",
    "    \"installation_id\",\n",
    "    \"title\", \n",
    "    \"type\",\n",
    "    \"world\",\n",
    "    \"pred_y\"\n",
    "] + list(set(train_df.columns) - set(test_df.columns)) + bad_feats\n",
    "\n",
    "\n",
    "train_cols = [c for c in list(train_df.columns) if c not in no_use_cols]\n",
    "    \n",
    "print(f\"train_df shape: {train_df.shape}\")\n",
    "print(train_cols)\n",
    "\n",
    "cat_cols = [\n",
    "            ]\n",
    "\n",
    "\n",
    "# logger.log(logging.DEBUG, f\"categorical cols: {cat_cols}\")\n",
    "\n",
    "target = \"accuracy_group\"\n",
    "\n",
    "model_conf = {\n",
    "    \"predict_type\": \"regressor\",\n",
    "    \"train_params\": train_params,\n",
    "    \"train_cols\": train_cols,\n",
    "    \"cat_cols\": cat_cols,\n",
    "    \"target\": target,\n",
    "    \"is_debug\": is_debug,\n",
    "}\n",
    "\n",
    "validation_param = {\n",
    "    \"model_name\": \"LGBM\",\n",
    "}\n",
    "\n",
    "exp_conf = {\n",
    "    \"train_small_dataset\": False,\n",
    "    \"use_feature\": {\n",
    "        \"sample\": True\n",
    "    },\n",
    "    \"train_params\": train_params,\n",
    "    \"exp_name\": exp_name\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "v = Validation(validation_param, exp_conf, train_df, test_df, logger)\n",
    "clf, oof, prediction, feature_importance = v.do_valid_kfold(model_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = prediction.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optR = OptimizedRounder()\n",
    "optR.fit(oof, train_df[target])\n",
    "coefficients = optR.coefficients()\n",
    "\n",
    "opt_preds = optR.predict(oof, coefficients)\n",
    "qwk(train_df[target], opt_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(oof).hist(bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(prediction).hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients\n",
    "\n",
    "prediction = optR.predict(prediction, coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.groupby(\"feature\")[\"importance\"].mean().reset_index().sort_values(\"importance\", ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sub_df =  pd.read_csv(base_path + \"/sample_submission.csv\")\n",
    "# prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df[\"accuracy_group\"] = prediction\n",
    "# .argmax(axis = 1)\n",
    "sub_df[\"accuracy_group\"].value_counts(normalize=True)\n",
    "sub_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "207px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
